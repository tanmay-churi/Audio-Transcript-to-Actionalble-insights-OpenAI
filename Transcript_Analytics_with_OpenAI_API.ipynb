{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC0t_IHQgV6-"
      },
      "source": [
        "#Create a Meeting Minute using OpenAI's GPT-3 from both Microsoft Team's Meeting Transcript or Zoom's Meeting Transcript (Transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7R-tsFJgr7j"
      },
      "source": [
        "This is my endeavor to replicate upcoming Microsoft Team Premium feature to create meeting notes using AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b_S-B5jg7f2"
      },
      "source": [
        "##1. Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfTwEED_g_kb"
      },
      "source": [
        "The following are prerequisites for this tutorial:\n",
        "\n",
        "- Python Package: openai\n",
        "- Python Package: torch and transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEIroo2BhEzH"
      },
      "source": [
        "###1.1. Python Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8G98yU0z8y_"
      },
      "source": [
        "####1.1.1. Install Python Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzLhnP--hOxp"
      },
      "source": [
        "##2. Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGFFGcY1tasJ"
      },
      "source": [
        "Colab code to prettify text output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ow3tBDBfseRE",
        "outputId": "f94880a8-846c-4500-d576-e1756118bd0d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmRPmprK0GM-"
      },
      "source": [
        "###2.1. Import Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "0Zs3zeEy0HXX",
        "outputId": "0fe7d7c4-f403-4580-a4f3-71fb6a489cf4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python:  3.10.11\n",
            "re:  2.2.1\n",
            "torch:  2.0.1+cpu\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'transformers' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mre: \u001b[39m\u001b[39m'\u001b[39m, re\u001b[39m.\u001b[39m__version__)\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtorch: \u001b[39m\u001b[39m'\u001b[39m, torch\u001b[39m.\u001b[39m__version__)\n\u001b[1;32m---> 15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtransformers:\u001b[39m\u001b[39m'\u001b[39m, transformers\u001b[39m.\u001b[39m__version__)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'transformers' is not defined"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "import os\n",
        "\n",
        "import openai\n",
        "\n",
        "import re\n",
        "from os.path import splitext, exists\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "print('Python: ', platform.python_version())\n",
        "print('re: ', re.__version__)\n",
        "print('torch: ', torch.__version__)\n",
        "print('transformers:', transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5EJ0NJHzmxx"
      },
      "source": [
        "###2.2. Mount Storage - Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Tx1d61zFzkMT",
        "outputId": "a8116f8c-c0b4-450e-8285-3ead7d3239a6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj6-FtO2zsb8"
      },
      "source": [
        "###2.3. Clean Meeting Transcript from either Microsoft Team or Zoom, encoded as WEBVTT file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvuyPQvdiTq1"
      },
      "source": [
        "The meeting transcript is encoded as follows:\n",
        "\n",
        "    WEBVTT\n",
        "    \n",
        "    03951482-18bc-403b-9a4f-9d2699587f03/65-1\n",
        "    00:00:08.885 --> 00:00:13.589\n",
        "    transcription making sure that\n",
        "    the transcription does work. Yep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW6bAm4WjRTz"
      },
      "source": [
        "This is not usually problem with ChatGPT, but OpenAI GPT-3 API charges by a token and we want to minimize the number of tokens sending to it. We will need to remove all lines that is not a transcript."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I6qHWX9kJb5"
      },
      "source": [
        "These two functions clean up .vtt file and then produce a clean text file with the same filename with an extension of .txt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "d1T3XitJ1Z58",
        "outputId": "6c399063-e9f6-4138-968f-a6fcbcb82319"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def clean_webvtt(filepath: str) -> str:\n",
        "    \"\"\"Clean up the content of a subtitle file (vtt) to a string\n",
        "\n",
        "    Args:\n",
        "        filepath (str): path to vtt file\n",
        "\n",
        "    Returns:\n",
        "        str: clean content\n",
        "    \"\"\"\n",
        "    # read file content\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as fp:\n",
        "        content = fp.read()\n",
        "\n",
        "    # remove header & empty lines\n",
        "    lines = [line.strip() for line in content.split(\"\\n\") if line.strip()]\n",
        "    lines = lines[1:] if lines[0].upper() == \"WEBVTT\" else lines\n",
        "\n",
        "    # remove indexes\n",
        "    lines = [lines[i] for i in range(len(lines)) if not lines[i].isdigit()]\n",
        "\n",
        "    # remove tcode\n",
        "    #pattern = re.compile(r'^[0-9:.]{12} --> [0-9:.]{12}')\n",
        "    pattern = r'[a-f\\d]{8}-[a-f\\d]{4}-[a-f\\d]{4}-[a-f\\d]{4}-[a-f\\d]{12}\\/\\d+-\\d'\n",
        "    lines = [lines[i] for i in range(len(lines))\n",
        "             if not re.match(pattern, lines[i])]\n",
        "\n",
        "    # remove timestamps\n",
        "    pattern = r\"^\\d{2}:\\d{2}:\\d{2}.\\d{3}.*\\d{2}:\\d{2}:\\d{2}.\\d{3}$\"\n",
        "    lines = [lines[i] for i in range(len(lines))\n",
        "             if not re.match(pattern, lines[i])]\n",
        "\n",
        "    content = \" \".join(lines)\n",
        "\n",
        "    # remove duplicate spaces\n",
        "    pattern = r\"\\s+\"\n",
        "    content = re.sub(pattern, r\" \", content)\n",
        "\n",
        "    # add space after punctuation marks if it doesn't exist\n",
        "    pattern = r\"([\\.!?])(\\w)\"\n",
        "    content = re.sub(pattern, r\"\\1 \\2\", content)\n",
        "\n",
        "    return content\n",
        "\n",
        "\n",
        "def vtt_to_clean_file(file_in: str, file_out=None, **kwargs) -> str:\n",
        "    \"\"\"Save clean content of a subtitle file to text file\n",
        "\n",
        "    Args:\n",
        "        file_in (str): path to vtt file\n",
        "        file_out (None, optional): path to text file\n",
        "        **kwargs (optional): arguments for other parameters\n",
        "            - no_message (bool): do not show message of result.\n",
        "                                 Default is False\n",
        "\n",
        "    Returns:\n",
        "        str: path to text file\n",
        "    \"\"\"\n",
        "    # set default values\n",
        "    no_message = kwargs.get(\"no_message\", False)\n",
        "    if not file_out:\n",
        "        filename = splitext(file_in)[0]\n",
        "        file_out = \"%s.txt\" % filename\n",
        "        i = 0\n",
        "        while exists(file_out):\n",
        "            i += 1\n",
        "            file_out = \"%s_%s.txt\" % (filename, i)\n",
        "\n",
        "    content = clean_webvtt(file_in)\n",
        "    with open(file_out, \"w+\", encoding=\"utf-8\") as fp:\n",
        "        fp.write(content)\n",
        "    if not no_message:\n",
        "        print(\"clean content is written to file: %s\" % file_out)\n",
        "\n",
        "    return file_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ZHOtHSt60g91",
        "outputId": "d9cfda74-5b4b-4a0d-da5e-68a78d0a3117"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clean content is written to file: follow up_2.txt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'follow up_2.txt'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filepath = \"follow up.vtt\"\n",
        "\n",
        "vtt_to_clean_file(filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC-sdhOqk7Qr"
      },
      "source": [
        "###2.4. Count the Number of Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgPybdZ1kqTR"
      },
      "source": [
        "OpenAI GPT-3 is limited by 4,001 tokens it can handle per request which includes both request (i.e., prompt) and response. We will be analyzing how many tokens are in this meeting transcript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "iy9QQE09E3cI",
        "outputId": "c5259411-6d5c-436a-a189-7fde599ed8ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def count_tokens(filename):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    with open(filename, 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
        "    num_tokens = input_ids.shape[1]\n",
        "    return num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "zyPjS1X-FpGh",
        "outputId": "152dbc36-ee34-4974-88df-6fa5ea9bc568"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5959 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens: 5959\n"
          ]
        }
      ],
      "source": [
        "filename = \"Acts Retirement-Life.txt\"\n",
        "token_count = count_tokens(filename)\n",
        "print(f\"Number of tokens: {token_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oeA8ivUmql8"
      },
      "source": [
        "###2.5. Break up the Meeting Transcript into chunks of 2000 tokens with an overlap of 100 tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNGJroB_mvCa"
      },
      "source": [
        "We will be breaking up the Meeting Transcript into chunks of 2,000 tokens with an overlapping 100 tokens to ensure any information is not lost from breaking up the meeting transcript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1WdNaxfbZUga",
        "outputId": "154a9703-49ad-4462-d84a-fcf1d5bc737a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def break_up_file_to_chunks(filename, chunk_size=2000, overlap=100):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    with open(filename, 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    tokens = tokenizer.encode(text)\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(0, num_tokens, chunk_size - overlap):\n",
        "        chunk = tokens[i:i + chunk_size]\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "xMQ4XMAzm05T",
        "outputId": "9e1d9ca9-5658-4411-895c-d75cf494c08b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5959 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk 0: 2000 tokens\n",
            "Chunk 1: 2000 tokens\n",
            "Chunk 2: 2000 tokens\n",
            "Chunk 3: 259 tokens\n"
          ]
        }
      ],
      "source": [
        "filename = \"Acts Retirement-Life.txt\"\n",
        "\n",
        "chunks = break_up_file_to_chunks(filename)\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i}: {len(chunk)} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdKOVSLcnH0J"
      },
      "source": [
        "###2.6. Validate the break up the Meeting Transcript into chunks of 2000 tokens with an overlap of 100 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "oKSR9cpj2JuQ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "mGd43PG3nvie",
        "outputId": "be8bfe8f-48e6-4e1c-9088-e355a1fbb8c4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " But Open AI just like you just spoke about, right, these use cases are are extremely pivotal in market right now where they are changing the whole experience design of users, you know in your industry like prescribers, subscribers you know to that capacity. So conversational AI, so that's where your natural language processing will play a big role and. You know, whether you're doing it through an automated fashion, whether you're doing it through voice, whether you're documenting it or you are\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(chunks[0][-100:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "hpkKJOqeJjx-",
        "outputId": "d30c8b28-e0f2-4a76-d578-884ffaaf75ab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " But Open AI just like you just spoke about, right, these use cases are are extremely pivotal in market right now where they are changing the whole experience design of users, you know in your industry like prescribers, subscribers you know to that capacity. So conversational AI, so that's where your natural language processing will play a big role and. You know, whether you're doing it through an automated fashion, whether you're doing it through voice, whether you're documenting it or you are\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(chunks[1][:100]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RplGPGmDFEph",
        "outputId": "040d7493-cd98-49d4-b12e-e118d0e22029"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overlap is Good\n"
          ]
        }
      ],
      "source": [
        "if tokenizer.decode(chunks[0][-100:]) == tokenizer.decode(chunks[1][:100]):\n",
        "    print('Overlap is Good')\n",
        "else:\n",
        "    print('Overlap is Not Good')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7ZEq816LCWr"
      },
      "source": [
        "###2.7. Set OpenAI API Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyDKuS7CLHJv"
      },
      "source": [
        "Please note that OpenAI's API service is not free, unlike ChatGPT demo. You will need to sign up for a service with them to get an API key, which requires payment information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gnYy6NrLJf7"
      },
      "source": [
        "Set an environment variable called “OPEN_API_KEY” and assign a secret API key from OpenAI (https://beta.openai.com/account/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GL4KCDfsK3mD",
        "outputId": "91c7fd41-584e-4e71-9ec3-6aa2e85c45e8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"API KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NALIE4VOLfEY",
        "outputId": "7a62576b-c407-4394-f0dd-bb5df0a53033"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LgrIt-an-7T"
      },
      "source": [
        "###2.8. Summarize the Meeting Transcript"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBSXqVbJom1N"
      },
      "source": [
        "####2.8.1. Summarize the Meeting Transcript one chunk at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oFfyvbntLxuF",
        "outputId": "8fe10ca1-0a92-491c-c5ff-1173dbe22a11"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5959 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "filename = \"Acts Retirement-Life.txt\"\n",
        "\n",
        "prompt_response = []\n",
        "prompt_tokens = []\n",
        "\n",
        "chunks = break_up_file_to_chunks(filename)\n",
        "for i, chunk in enumerate(chunks):\n",
        "    prompt_request = \"Summarize this meeting transcript: \" + tokenizer.decode(chunks[i])\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "            model=\"text-davinci-003\",\n",
        "            prompt=prompt_request,\n",
        "            temperature=.5,\n",
        "            max_tokens=500,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0\n",
        "    )\n",
        "\n",
        "    prompt_response.append(response[\"choices\"][0][\"text\"])\n",
        "    prompt_tokens.append(response[\"usage\"][\"total_tokens\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SsbvQ19pSKw5",
        "outputId": "ff5a8976-8ec6-43b3-a51b-46f2cffd9fbf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[\", you know, pushing it out to the other people, that's where the whole piece comes into play.\\n\\nSam and Richard discussed the capabilities of Sam's company, which includes a network of 50,000 contractors with expertise in AI, cloud, data, and app development. They discussed the upcoming changes to Microsoft's partnership program, the company's experience in AI, and how their services could help PDHI with automation, natural language processing, voice recognition, and data fabric.\",\n",
              " \" captive team model is that you are basically saying, hey, you know, I need you to do this and then I need you to do this and then I need you to do this. So the whole idea of the captive model is that you are able to basically get the same team to work on different type of projects. So if you're saying that I'm looking at the Salesforce cloud, I'm looking at the Microsoft cloud, I'm looking at the Google cloud, and I'm looking at the AWS cloud. I'm able to basically get the same team to work on all those projects and then I'm able to make a more informed decision.\\n\\nThis meeting discussed the use of Open AI in the market and how it is changing the experience design for users. The use cases discussed included conversational AI, summarizing, and creating bullet points. The discussion then moved on to Microsoft's Dataverse, Power BI, and other tools. The speaker then discussed the different models of working with their company, including the captive unit, the competent studio, and the core flexi model. The core flexi model was highlighted as the most successful model, as it allows for a team extension and cross education.\",\n",
              " \" to do some kind of research or anything like that, please let me know. I'm more than happy to help you out with that.\\n\\nThis meeting concluded with a plan to have an in-person meeting on either the 24th or 26th of July at 11:30. The meeting will include Peter and Chris and will involve a discussion about how to accelerate and be more efficient in getting projects moving. Sam will be sending an invite and an NDA ahead of time. Flex and the captive team could be helpful with evaluating software and there is the potential to get supplemental people like business analysts and data analysts. There is a cost advantage with the company as they offer experienced people at a rate of 35-40 dollars an hour.\",\n",
              " '\\n\\nSam and Richard had a meeting to discuss an upcoming event. Sam is sending over a block of emails and asked Richard if he had any specific use cases that the team should be prepared for. Richard thanked Sam and they both agreed to look forward to the invite and to take care of themselves.']"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tG4qzIMDyGeU",
        "outputId": "7d0e5590-9d53-4a53-a294-6e6b11af7a0e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[2104, 2252, 2156, 326]"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zNYP54XJyK6d",
        "outputId": "4c58e634-23da-435f-e6a5-02a7e433c53d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sum of all elements in given list:  6838\n"
          ]
        }
      ],
      "source": [
        "total = 0\n",
        "\n",
        "for e in range(0, len(prompt_tokens)):\n",
        "    total = total + prompt_tokens[e]\n",
        "\n",
        "print(\"Sum of all elements in given list: \", total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL1451Igozer"
      },
      "source": [
        "####2.8.2. Consolidate the Meeting Transcript Summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5eh0cVS0PW5_",
        "outputId": "fe5374ba-4658-4f73-bf45-971a2c96b273"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt_request = \"Consoloidate these meeting summaries: \" + str(prompt_response)\n",
        "\n",
        "response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=prompt_request,\n",
        "        temperature=.5,\n",
        "        max_tokens=1000,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "yu_cQjLDTAB4",
        "outputId": "f5fcc175-baab-4c4b-aded-a3b6cb9a8217"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Sam and Richard discussed the capabilities of Sam's company, which includes a network of 50,000 contractors with expertise in AI, cloud, data, and app development. They discussed the upcoming changes to Microsoft's partnership program, the company's experience in AI, and how their services could help PDHI with automation, natural language processing, voice recognition, and data fabric. The use cases discussed included conversational AI, summarizing, and creating bullet points, as well as Microsoft's Dataverse, Power BI, and other tools. The speaker then discussed the different models of working with their company, including the captive unit, the competent studio, and the core flexi model. The core flexi model was highlighted as the most successful model, as it allows for a team extension and cross education. The meeting concluded with a plan to have an in-person meeting on either the 24th or 26th of July at 11:30. The meeting will include Peter and Chris and will involve a discussion about how to accelerate and be more efficient in getting projects moving. Sam will be sending an invite and an NDA ahead of time. Flex and the captive team could be helpful with evaluating software and there is the potential to get supplemental people like business analysts and data analysts. There is a cost advantage with the company as they offer experienced people at a rate of 35-40 dollars an hour.\n"
          ]
        }
      ],
      "source": [
        "meeting_summary = response[\"choices\"][0][\"text\"]\n",
        "print(meeting_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRwpgJ1gpPhL"
      },
      "source": [
        "###2.9. Get Action Items from Meeting Transcript"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhZfit-nplpx"
      },
      "source": [
        "####2.9.1. Get Action Items from the Meeting Transcript one chunk at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "JvHJV8KeUzeS",
        "outputId": "460fe225-0c30-4d5b-dd56-5f6f37fdfe3e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (18637 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "filename = \"follow up.txt\"\n",
        "\n",
        "action_response = []\n",
        "action_tokens = []\n",
        "\n",
        "chunks = break_up_file_to_chunks(filename)\n",
        "for i, chunk in enumerate(chunks):\n",
        "    prompt_request = \"Provide a list of important action items with a due date from the provided meeting transcript text: \" + tokenizer.decode(chunks[i])\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "            model=\"text-davinci-003\",\n",
        "            prompt=prompt_request,\n",
        "            temperature=.5,\n",
        "            max_tokens=500,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0\n",
        "    )\n",
        "\n",
        "    action_response.append(response[\"choices\"][0][\"text\"])\n",
        "    action_tokens.append(response[\"usage\"][\"total_tokens\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZNzPeL8SVJSG",
        "outputId": "50397550-c3ba-4632-b4d6-5d4d21c43162"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['af3-f945f3d9f2f2-0\\n\\nAction Items: \\n1. Schedule call with Peter to designate tasks and resources - Due Date: TBD \\n2. Look at website and research organization - Due Date: ASAP \\n3. Move more north - Due Date: TBD', \"107abc90-6 then if it's something else, then it can go to a person.\\n\\nAction Items:\\n\\n1. Source contractors to help organization - Due Date: N/A\\n2. Become a service based partner with Microsoft - Due Date: July \\n3. Spearhead AI related efforts - Due Date: N/A\\n4. Make ways to do more automation - Due Date: N/A\\n5. Implement AI ticketing system - Due Date: N/A\", \" think that's what it's called.\\n\\nAction Items: \\n1. Create a ticketing system to respond to common questions and issues with a due date of April 1st. \\n2. Research and create training for AI with a due date of April 15th. \\n3. Explore natural language processing and voice recognition with a due date of April 30th. \\n4. Investigate Salesforce Health Cloud versus Microsoft Health with a due date of May 15th. \\n5. Pull Apple health data using different vendors with a due date of May 30th. \\n6. Analyze voice to text narrative notes with a due date of June 15th. \\n7. Research data fabric and Azure ML OPS with a due date of June 30th.\", \"c00c7-faab-4d31-9ca6-95e34329db52-4 the enterprise.\\n\\nAction Item 1: Research Azure ML OPS and create a pipeline for scaling out tenants - Due Date: 2 weeks \\nAction Item 2: Investigate Open AI use cases and create an experience design for users - Due Date: 3 weeks \\nAction Item 3: Develop a natural language processing system for conversational AI - Due Date: 4 weeks \\nAction Item 4: Create a workflow for automated decision making - Due Date: 5 weeks \\nAction Item 5: Research Microsoft Dataverse, Data OPS, Power BI, and Orca tools - Due Date: 1 week \\nAction Item 6: Investigate large language models for data fabric aspect - Due Date: 2 weeks \\nAction Item 7: Research Microsoft's Build with Microsoft project - Due Date: 1 week\", 'fdc56fe-c40b-412d-9daf-6fe57671ec7d-3 resources from the market to come in and supplement the 0fdc56fe-c40b-412d-9daf-6fe57671ec7d-4 existing teams.\\n\\nAction Item 1: Investigate Salesforce Cloud vs. Microsoft Health Crowd - Due Date: August \\n\\nAction Item 2: Hire a Techno Functional Architect to Validate and Create Course of Action - Due Date: TBD \\n\\nAction Item 3: Invest in One Resource with Experience in Both Apps or Ecosystems - Due Date: TBD \\n\\nAction Item 4: Consider Competent Studio Model - Due Date: TBD \\n\\nAction Item 5: Supplement Existing Teams with Resources from Market - Due Date: TBD \\n\\nAction Item 6: Attend Webinar Hosted by Microsoft and Us - Due Date: August', \"-3 we know the way you do things. So we are able to source c9893bd8-5fc0-4fae-9543-b57808e0bdaa-4 people very quickly and ramp up your team.\\n\\nAction Item 1: Source 20 skill sets for Sam and Richard's team by April 15th. \\n\\nAction Item 2: Create architecture, wireframes, and a proof of concept by May 1st.\\n\\nAction Item 3: Provide QA services, demos for OPS services, and devop services by June 15th.\\n\\nAction Item 4: Deliver 20 tasks or solutions and train customers while doing it by July 1st. \\n\\nAction Item 5: Create core team of 5 people, 2 people, or more depending on company size by August 15th. \\n\\nAction Item 6: Source an architect with open term by September 1st. \\n\\nAction Item 7: Ramp up and ramp down team quickly and retain knowledge of processes, culture, and the way things are done by October 15th.\", \"8124-49cb-8c5c-6c1c1a740382-1 being in the same office, it's not a problem. And like I said 4deec6b8-8124-49cb-8c5c-6c1c1a740382-2 we also have the ability to ramp up and ramp down very 4deec6b8-8124-49cb-8c5c-6c1c1a740382-3 quickly.\\n\\nAction Item #1: Ramp up teams quickly - Due Date: ASAP \\nAction Item #2: Retain knowledge of processes - Due Date: ASAP \\nAction Item #3: Align with company culture - Due Date: ASAP \\nAction Item #4: Source requirements quickly - Due Date: ASAP \\nAction Item #5: Evaluate software - Due Date: ASAP \\nAction Item #6: Create blueprint - Due Date: 2 months \\nAction Item #7: Source long-term contractors - Due Date: ASAP\", 'b09-0\\n\\nImportant Action Items:\\n\\n1. Discuss the best of both worlds and cost advantages of offshore employees with Peter and Chris, due date: Week of 10th \\n2. Meet in person with Peter and Chris, due date: To be discussed \\n3. Discuss long-term relationships and repeat orders with Peter and Chris, due date: To be discussed \\n4. Prove value to Peter and Chris with a $5000 project, due date: To be discussed', 'ee8962b511-1 if I invited the data team to join us? Yeah, that would be 8246a251-e6fa-40f2-8716-84ee8962b511-2 great.\\n\\nAction Items:\\n\\n1. Schedule meeting with Richard and Sam for Monday 11:30am - 1:00pm on week of 10th or 24th July - Due Date: Friday 5th July\\n\\n2. Book Florida room for meeting - Due Date: Friday 5th July\\n\\n3. Invite Data Team to join meeting - Due Date: Friday 5th July', '\\n\\n1. Richard and Sam to send ND or some kind of MSA ahead of time: Due Date: July 24th or 26th \\n2. Sam to send invite for in-person meeting: Due Date: July 24th or 26th \\n3. Richard and Sam to provide any specific use cases of high priority: Due Date: Before July 24th or 26th']\n"
          ]
        }
      ],
      "source": [
        "print(action_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Od7s1fwpdy3"
      },
      "source": [
        "####2.9.2. Consolidate the Meeting Transcript Action Items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "MISAIRbJWamn",
        "outputId": "9c430906-9c8c-400d-a69b-43af077960e9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt_request = \"Consolidate these meeting action items, but exclude action items with Due Date of Immediately: \" + str(action_response)\n",
        "\n",
        "response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=prompt_request,\n",
        "        temperature=.5,\n",
        "        max_tokens=500,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "action_tokens= response[\"usage\"][\"total_tokens\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "5b3aowAFWjdY",
        "outputId": "a1dea41d-3849-402d-d92e-2893e64cfe5e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Action Items: \n",
            "1. Schedule call with Peter to designate tasks and resources - Due Date: TBD \n",
            "2. Research and create training for AI - Due Date: April 15th \n",
            "3. Explore natural language processing and voice recognition - Due Date: April 30th \n",
            "4. Investigate Salesforce Health Cloud vs. Microsoft Health - Due Date: May 15th \n",
            "5. Pull Apple health data using different vendors - Due Date: May 30th \n",
            "6. Analyze voice to text narrative notes - Due Date: June 15th \n",
            "7. Research Azure ML OPS and create a pipeline for scaling out tenants - Due Date: 2 weeks \n",
            "8. Investigate Open AI use cases and create an experience design - Due Date: 3 weeks \n",
            "9. Develop a natural language processing system for conversational AI - Due Date: 4 weeks \n",
            "10. Create a workflow for automated decision making - Due Date: 5 weeks \n",
            "11. Research Microsoft Dataverse, Data OPS, Power BI, and Orca tools - Due Date: 1 week \n",
            "12. Invest in language models for data fabric aspect - Due Date: 2 weeks \n",
            "13. Research Microsoft's Build with Microsoft project - Due Date: 1 week \n",
            "14. Investigate Salesforce Cloud vs. Microsoft Health Crowd - Due Date: August \n",
            "15. Hire a Techno Functional Architect to Validate and Create Course of Action - Due Date: TBD \n",
            "16. Invest in One Resource with Experience in Both Apps or Ecosystems - Due Date: TBD \n",
            "17. Consider Competent Studio Model - Due Date: TBD \n",
            "18. Supplement Existing Teams with Resources from Market - Due Date: TBD \n",
            "19. Attend Webinar Hosted by Microsoft and Us - Due Date: August \n",
            "20. Source 20 skill sets for Sam and Richard's team - Due Date: April 15th \n",
            "21. Create architecture, wireframes, and a proof of concept - Due Date: May 1st \n",
            "22. Provide QA services, demos for OPS services, and devop services - Due Date: June 15th \n",
            "23. Deliver 20 tasks or solutions and train customers while doing it - Due Date: July 1st \n",
            "24. Create core team of 5 people, 2 people, or more depending on company size - Due Date: August 15th \n",
            "25. Source an architect with open term - Due Date:\n"
          ]
        }
      ],
      "source": [
        "meeting_action_items = response[\"choices\"][0][\"text\"]\n",
        "print(meeting_action_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G3IG4OdOmZ5n",
        "outputId": "57e7b7c4-43f3-4eef-b6ce-fecb08d1f695"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "1533"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"usage\"][\"prompt_tokens\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l8w402OGnGCZ",
        "outputId": "b5379619-a317-4124-dd37-526780b2d1af"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"usage\"][\"completion_tokens\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hBJ2kbRBnJGY",
        "outputId": "289eeeb6-2107-42d6-bd16-7f6b18326df1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "2033"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"usage\"][\"total_tokens\"]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
